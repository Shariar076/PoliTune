>>>>>>>>>> RUNNING FT FOR 100R0L <<<<<<<<<<
Running LoRADPORecipeSingleDevice with resolved config:

batch_size: 8
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: checkpoints/Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: checkpoints/
  recipe_checkpoint: null
compile: false
dataset:
  _component_: dataset.politune_right_pref
device: cuda
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 4
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_peak_memory_stats: true
loss:
  _component_: torchtune.rlhf.loss.DPOLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 1000
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: outputs/logs
model:
  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 32
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 16
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: outputs
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: null
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 1024
  path: checkpoints/Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Setting manual seed to local seed 321204752. Local seed is seed + rank = 321204752 + 0
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.09 GiB
	GPU peak memory reserved: 15.21 GiB
	GPU peak memory active: 15.09 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss function is initialized.
Writing logs to outputs/logs/log_1748463562.txt
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10317 examples [00:00, 23172.35 examples/s]Generating train split: 10317 examples [00:00, 22535.37 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
