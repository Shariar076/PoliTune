>>>>>>>>>> RUNNING FT FOR 100R0L <<<<<<<<<<
Running LoRADPORecipeSingleDevice with resolved config:

batch_size: 8
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: checkpoints/Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: checkpoints/
  recipe_checkpoint: null
compile: false
dataset:
  _component_: dataset.politune_right_pref
device: cuda
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 4
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_peak_memory_stats: true
loss:
  _component_: torchtune.rlhf.loss.DPOLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 1000
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: outputs/logs
model:
  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 32
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 16
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: outputs
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: null
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 1024
  path: checkpoints/Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Setting manual seed to local seed 1806993647. Local seed is seed + rank = 1806993647 + 0
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	GPU peak memory allocation: 15.09 GiB
	GPU peak memory reserved: 15.21 GiB
	GPU peak memory active: 15.09 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss function is initialized.
Writing logs to outputs/logs/log_1748462628.txt
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10317 examples [00:00, 75685.86 examples/s]Generating train split: 10317 examples [00:00, 75505.60 examples/s]
Dataset and Sampler are initialized.
Learning rate scheduler is initialized.
  0%|          | 0/161 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/skabi9001/.local/bin/tune", line 8, in <module>
    sys.exit(main())
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/_cli/tune.py", line 52, in main
    parser.run(args)
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/_cli/tune.py", line 46, in run
    args.func(args)
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/_cli/run.py", line 214, in _run_cmd
    self._run_single_device(args, is_builtin=is_builtin)
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/_cli/run.py", line 108, in _run_single_device
    runpy.run_path(str(args.recipe), run_name="__main__")
  File "/usr/lib/python3.10/runpy.py", line 289, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/usr/lib/python3.10/runpy.py", line 96, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/skabi9001/.local/lib/python3.10/site-packages/recipes/lora_dpo_single_device.py", line 646, in <module>
    sys.exit(recipe_main())
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/config/_parse.py", line 99, in wrapper
    sys.exit(recipe_main(conf))
  File "/home/skabi9001/.local/lib/python3.10/site-packages/recipes/lora_dpo_single_device.py", line 641, in recipe_main
    recipe.train()
  File "/home/skabi9001/.local/lib/python3.10/site-packages/recipes/lora_dpo_single_device.py", line 526, in train
    for idx, batch in enumerate(self._dataloader):
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/datasets/_preference.py", line 129, in __getitem__
    return self._prepare_sample(sample)
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/datasets/_preference.py", line 132, in _prepare_sample
    transformed_sample = self._message_transform(sample)
  File "/home/skabi9001/.local/lib/python3.10/site-packages/torchtune/data/_messages.py", line 331, in __call__
    if message["role"] == "system" and self.new_system_prompt is not None:
TypeError: string indices must be integers
  0%|          | 0/161 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/skabi9001/.local/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
  File "/home/skabi9001/.local/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py", line 59, in main
    service.run()
  File "/home/skabi9001/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py", line 207, in run
    print(self._upload())
  File "/home/skabi9001/.local/lib/python3.10/site-packages/huggingface_hub/commands/upload.py", line 267, in _upload
    raise FileNotFoundError(f"No such file or directory: '{self.local_path}'.")
FileNotFoundError: No such file or directory: 'checkpoints/epoch_3'.
